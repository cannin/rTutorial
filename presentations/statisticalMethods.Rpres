Introduction to Statistical Methods
===
author: Augustin Luna
date: `r format(Sys.time(), '%d %B, %Y')`
width: 960
height: 700
transition: linear
css: rpres.css

<!-- NOTE: Styling and external images may be missing --> 

<p>Research Fellow
  <br/>
  Department of Biostatistics and Computational Biology
  <br/>
  Dana-Farber Cancer Institute
</p>
<div class="footer" style="display:none;"><img src="img/dfci_logo.gif" height="60px" width="330px" /></div>

Topics to be Covered
===
* Correlations
* Enrichment Analyses
* Multiple Testing Correction
* Regression
* Clustering
* Dimensionality Reduction

Introduction
===
* Data analyses are the product of many tasks
* Data preparation
 * Extracting structured data from unstructured data sources 
 * Merging data sources 
 * Ensuring consistency of datasets 
* Dataset interpretation 
 * Create visualizations to present and communicate findings
* Statistics 
 * Build predictive mathematical models 
* Methods are common in the areas of informatics, data mining, data science, machine learning, and statistics 

Statistical Methods Flowchart
===
class: center-img, smaller

* The flowchart below helps find the right method for a given problem 
 * http://scikit-learn.org/stable/tutorial/machine_learning_map/
 
<img src="img/ml_map.png" height="500px" />

Methods to be Covered
===
* Basic method will be covered to build confidence with R and the general concepts
 * Dimensionality Reduction: Principal Component Analysis (PCA)
 * Regression: Linear regression
 * Clustering: Hierarchical and K-means clustering
 * Classification will not be covered

Correlating Two Vectors
===
class: smaller-75

```{r}
# Make sure the random numbers are always the same 
set.seed(1)

# Generate two sets of 20 random numbers
a <- runif(20); b <- runif(20)

# Calulate the correlation of the two sets
cor.test(a, b)
```

Extracting Values From Results
===
class: smaller-75

* Values in results are described in the help `?cor.test`
* A p-value is the probability of seeing results as extreme as the ones produced in an analysis.

```{r}
set.seed(1)

a <- runif(20)
b <- runif(20)

results <- cor.test(a, b, method="pearson")
names(results)
results$p.value
```

Over-Representation (ORA) and Enrichment Analyses
===
class: smaller

* Enrichment tests are widely used in biology to determine if the genes contain a trait more frequently than a random sampling of genes
 * Gene Ontology (GO) term (e.g. biological process, molecular function, or cellular component) and pathways are the most common comparisons made
* Several tools exist for doing enrichment analyses
 * The tests are either Fisher's exact test or hypergeometric test; these tests produce the same results
 * These calculations can be done in R using `fisher.test` and `phyper`

Enrichment Analysis 
===
class: smaller 

* What is the probability of randomly drawing at least 4 black points in a random sample of 5 points? 
 * The concept of "black" could be replaced by "genes from a given pathway" or "genes with a common function"

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=4, dpi=150, out.height="600px"}
set.seed(2)

size <- 50

x <- runif(size)
y <- runif(size)

#clazz <- sample(c("black", "red"), size, replace=TRUE)
clazz <- c(rep("black", 5), rep("red", 45))

df <- data.frame(x=x, y=y, class=clazz)

par(mai=c(1.2,0,0,0))
plot(df$x, df$y, pch=21, bg=c("black", "red")[unclass(df$class)], xlab="", ylab="", axes=FALSE)
```
 
Calculating an ORA (Enrichment) P-Value
===
class: smaller

The significance of an over-representation (enrichment analysis) is calculated using a hypergeometric test:

$$
\begin{align}
P(pathway_j) = 1 - \sum_{i=0}^{k-1}{\frac{\binom{K}{i}\binom{N-K}{n-i}}{\binom{N}{n}}}
\end{align}
$$

where 

* N: number of studied genes, 
* n: total number of genes identified by a previous analysis
* K: total number of genes in a pathway $pathway_j$
* k: number genes previously identified genes in $pathway_j$
* $\binom{n}{k}$: choose $k$ elements from a set with $n$ elements disregarding order

The p-value of this test indicates the probability that a random selection of genes of the same size as the input gene set from a population would produce the same number of observed annotations (e.g. for a specific GO term or pathway) or more in the gene set 
 
Choose k from N Without Order
===
class: smaller-75
left: 75%

```{r}
N <- 5
k <- 3
factorial(N) / (factorial(k)*factorial(N-k))
```

***

```{r}
N <- 5
k <- 3
choose(N, k)
```
 
Contingency Table for Enrichment Analysis
===

|       | Drawn | Not Drawn  | Total   | 
|-------|-------|------------|---------|
| Black | k=4   | K-k=1      | K=5     |
| Red   | n-k=6 | N+k-n-K=39 | N-K=45  |
| Total | n=10  | N-n=40     | N=50    |

phyper or fisher.test Example
===
class: smaller-75

* NOTE: `hitInSample-1` is nescessary in `phyper` beacuse if `lower.tail` is FALSE, probabilities returned are P(X > x). Subtract x by 1 to get P(X â‰¥ x) (x equal to or greater than).

```{r}
sampleSize <- 10 # size drawn
hitInSample <- 4 # black drawn
hitInPop <- 5 # all black 
failInPop <- 50-hitInPop # number of red

phyper(hitInSample-1, hitInPop, failInPop, sampleSize, lower.tail= FALSE);
fisher.test(matrix(c(hitInSample, hitInPop-hitInSample, sampleSize-hitInSample, failInPop-sampleSize +hitInSample), 2, 2), alternative='greater')$p.value; 
```

Multiple Testing Correction for Enrichment Analyses
===
* Enrichment analyses often do analyses over a large number of molecular functions or pathways 
* If we conduct many such tests, we are likely to see false positives 
 * A p-value significance cutoff of 0.05 means that we expect 1 test out of 20 to appear significant by random chance (i.e. a false positive) 

Multiple Test Corrections Types
===
* Family-Wise Error Rate (FWER): Controls the probability that any test is a false positive
 * Bonferroni Correction: Very stringent correction
 * Adjusted p-value = original p-value * number of tests
* False Discovery Rate (FDR): Controls the proportion of tests that are false positives 
 * Widely used alternative to FWER (e.g. Bonferroni correction)
 * Example (next slide)

Benjamini-Hochberg (BH) FDR Procedure
===
* Goal: Calculate the new p-value cutoff for a given set of p-values
* Assume $n = 100, \alpha = 0.05$
 * $\alpha$ denotes the desired false positive rate, 
 
1. Rank $n$ p-values (large to small)
2. Calculate q-values 
 $$
 \begin{align}
 	q = \alpha \times \frac{n-rank+1}{n}
 \end{align}
 $$
3. Select the lowest ranked p-value that is lower than $\alpha$

Benjamini-Hochberg Example 
===

| p-value | Rank | q-value | p < q |
|-----|------|---| ----- |
| 0.9 | 1 | 0.05*(100-1+1)/100=0.05 | FALSE | 
| 0.7 | 2 | 0.05*(100-2+1)/100=0.0495 | FALSE | 
| 0.5 | 3 | 0.05*(100-3+1)/100=0.049 | FALSE | 
| 0.04 | 4 | 0.05*(100-4+1)/100=0.0485 | TRUE | 
| ... | ... | ... | ... |
| 0.005 | n | 0.05*(100-n+1)/100=5E-4 | FALSE | 

Bonferroni and Benjamini-Hochberg Corrections in R 
===
class: smaller-75

```{r, echo=FALSE}
options(scipen=999)
```

```{r}
pVals <- read.table("files/pvalsExample.txt")
head(pVals$V1, 5)

pValsAdjusted <- p.adjust(pVals$V1, method="bonferroni")
head(pValsAdjusted, 5)

# 'fdr' or 'BH' for Benjamini-Hochberg method
pValsAdjusted <- p.adjust(pVals$V1, method="fdr")
head(pValsAdjusted, 5)
```

```{r, echo=FALSE}
options(scipen=0)
```

Additional Enrichment Analyses
===
* Gene Set Enrichment Analysis (GSEA): GSEA is one of the best known enrichment analyses
 * This method additionally takes into account numeric values associated with the genes (e.g. gene expression levels)
 * They provide many collections of "gene sets" that can be used with GSEA or related methods
 * http://software.broadinstitute.org/gsea/msigdb

Regression
===
class: smaller-75

* Goal: Find the relationship between an independent variable and a set of dependent variables (also known as predictor or features)
 * Example: The relationship between the expression some genes and drug response

Given $n$ observations each with a response variable $y$ and $p$ predictors (or features)

$$
\begin{align}
Y & = (y_1, \ldots ,y_n)^T, \hspace{1em} n \times 1\\
X & = (X_1, \ldots ,X_p), \hspace{1.2em} n \times p
\end{align}
$$

Goal: We want to find a set of regression coefficients $\beta$ for $x = (x_1, \ldots, x_p)$ to describe the relationship between $y$ and $x_1, \dots, x_p$

$$\hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \ldots$$

* $\hat{y}$ is the predicted value 
* $\hat{\beta}$ are the estimated regression coefficients (as opposed to the true coefficients)

Example Regression
===
class: smaller-75
```{r}
fit <- lm(Petal.Width ~ Petal.Length, data=iris)
summary(fit)
```

Interpreting lm() Results Summary
===
class: smaller
* Residuals: The difference between the actual and predicted values
* Estimate: Regression coefficient estimates
* Std. Error: Measurement of the variability of the coefficient estimate. Lower is better. 
* t value: Coefficient score to describe the importance of predictor; used to calculate the p-value
* Pr(>|t|): Coefficient p-value. Probability the predictor is **NOT** relevant
* R-square: Score for evaluating how well the model fits the data. Higher is better. This can be adjusted for the number of predictors used in the model. 
 * Values ~0.7 are of more interest, but there is no standard rule
* More information: http://blog.yhat.com/posts/r-lm-summary.html

Some Issues with Regression
===
class: smaller 

* Missing Data: What if your data has missing values? 
 * Imputation can be used to fill in missing values using other data points
* Too many predictors versus the number of samples
 * The "Curse of Dimensionality" (later slide)
 * Regularized regression methods can be used to select features to be included in the model
* Overfitting: Will your model work on other datasets? 
 * Excessively complex models (e.g. having too many predictors) can have poor predictive performance when tested on new data
 * Regularized regression methods have properties to avoid overfitting

Missing Data 
===
* By default, `lm()` removes rows that contain missing values 
* An alternative is imputation to fill in missing values
 * `impute` imputes using K-Nearest Neighbors (KNN) 
 * Step 1: Identify K number of neighbors based on Euclidean distance
 * Step 2: Average the values of the neighbors and replace the missing value

Basic Rules for Imputation
===
* Should be done when the number of missing values is small
 * A safe maximum threshold is 5% of the total for large datasets
* Should be done when the imputed values are plausible for the missing values
* Should be done when it is assumed that the missing values occur at random
 * If missing values do not occur at random, the data collection should be investigated
 and/or the values should be dropped

The "Curse of Dimensionality"
===
class: smaller 

* If the number of predictors is greater than the number of samples, it will not be possible to estimate relevant regression parameters in the full model. 
 * This is due to the degrees of freedom in the system. 
 * There are $n$ observations and $p+1$ parameters (one regression coefficient for each predictor plus the intercept) leaving $n-p-1$ degrees of freedom.
 * Increasing the sample size provides more information about the population test. 
 * Increasing the number of predictors in the resulting model lowers the degrees of freedom available to estimate the variability of the predictors; this increases the variance of the regression coefficient estimates and reduces confidence in the model.

LASSO, Ridge, and Elastic Net Regularized Regression
===
class: smaller

* Least Absolute Shrinkage and Selection Operator (LASSO): Tends to produce sparse (i.e. few predictors) whereby the algorithm selects an arbritary predictor among a set of correlated ones 
* Ridge: Tends to select all correlated predictors with their coefficient values equal to each other
* Elastic Net: Blends the concepts of LASSO and Ridge regression to attempt to create a model that is both sparse, but also includes correlated predictors
* LASSO, Ridge, and Elastic Net regression are available from the `glmnet` R package
 * Sousa FG et al. DNA damage response alterations and its relation with drug activity across the NCI-60. DNA Repair (2015) for example usage of `glmnet` and Elastic Net

Clustering 
=== 
* Goal: Divide data into groups (clusters), so that group members are more "similar" to each other than to members outside the group
 * Example: Cluster a set of drugs. For drugs without a known mechanism of action (MOA), predict a potential MOA based on how the unknown MOA drugs cluster with known MOA drugs
* Clustering differs from classification in that in classification we have known groups 

Hierarchical Clustering 
===

* R uses an agglomerative (bottom-up) clustering approach 
 * Alternative: Divise (top-down) is similiar to agglomerative, but in reverse
* Algorithm 
 1. All points start in their own clusters 
 2. At each iteration merge the 2 most similar structures
 3. Stop if there is a single cluster containing all points, else go to Step 2

Hierarchical Clustering Example
===
class: smaller-50

* Heatmap shows the expression of 20 oncogenes from 20 NCI-60 celllines

```{r, fig.align="center", fig.height=4, fig.width=4, dpi=150, out.height="500px"}
dat <- read.table("files/heatmapExample.txt", sep="\t", header=TRUE)
mat <- as.matrix(dat); heatmap(mat, cexCol=0.75)
```

Cluster Similarity (Linkage)
===
* Distances between clusters are calculated to determine cluster similarity
* Options of Cluster Linkage Distances
 * Single: Distance between two clusters is defined by the distance between the two closest points. 
 * Average: Average of all pairwise distances between the points in two clusters
 * Complete: Distance between two clusters is defined by the distance between the two farthest points. 
* `hclust()` used by `heatmap()` in R uses the "complete" method by default

K-Means Clustering 
===
* Algorithm 
 1. A user-selected ($k$) number of means are randomly generated from the data 
 2. $k$ clusters are created by grouping data points to the nearest mean. 
 3. The centroid of the clusters becomes the new mean.
 4. Steps 2 and 3 are repeated until the clusters do not change anymore

K-Means Clustering Example
===
class: smaller-50

```{r, fig.align="center", fig.height=4, fig.width=4, dpi=150, out.height="450px"}
# Retain only the numeric data in the iris dataset
iris_data <- iris[, 1:4]

kc <- kmeans(iris_data, 3)

par(mai=c(1,1,0,0))
plot(iris[c("Sepal.Length", "Sepal.Width")], bg=c("red","green3","blue")[kc$cluster], pch=21)
points(kc$centers[,c("Sepal.Length", "Sepal.Width")], col=c("red","green3","blue"), pch=8, cex=2)
```

Compare the Species with Clustering
===

```{r}
table(iris$Species, kc$cluster)
```  

Determine K-Means Cluster Quality 
===
class: smaller-75
left: 50%

```{r}
library(cluster)
dataDist <- dist(iris_data)
si <- silhouette(kc$cl, dataDist)
```
* Silhouette Plot
 * Horizontal barplot is the goodness of fit of sample within the cluster
 * Longer is better
 * Rightmost number, $S_i$, is average length
* Average Silhouette Guidelines
 * 0.71-1.0: Strong clustering
 * 0.51-0.70: Reasonable clustering
 * < 0.50: Weak clustering
 
***

```{r, fig.align="center", fig.height=8, fig.width=8, dpi=150, out.height="400px"}
plot(si, col = c("red", "green3", "blue"))
```

Selecting k with Average Silhouette
===
class: smaller-75

```{r}
library(cluster)
kMax <- 15
avgSi <- rep(0, kMax)

# Average silhouette width
# k: 2 to 15
for(i in 2:kMax){
  results <- kmeans(iris_data, centers=i)
  si <- silhouette(results$cluster, dist(iris_data))
  avgSi[i] <- mean(si[, "sil_width"])
}
```
*** 
```{r, fig.align="center", fig.height=8, fig.width=8, dpi=150, out.height="400px"}
plot(1:kMax, avgSi, type="b", pch=19, xlab="Number of Clusters (k)")
abline(v=which.max(avgSi), lty=2)
```

Differences between Hierarchical and K-Means Clustering
===
* Clusters 
 * K-means produces a single set of clusters
 * Hierarchical produces different clusters depending on where the tree is cut
* Cluster Number 
 * K-means requires the number clusters to be set
 * Hierarchical clustering  does not require the number of clusters to be set
* Speed
 * K-means is faster than hierarchical clustering

Dimensionality Reduction
===
* Goal: Seeks to reveal correlations that exist in data with many predictors
 * This can be used to reduce the dimensionality of the data by decreasing the redundancy of correlated variables 

* Results: 
 * Loadings: weight for the original variables to be multiplied by to get the component scores 
 * Component Scores: Transformed variable values for a given point
* `prcomp` and `princomp` can do PCA in R; `prcomp` is the advised function

What is Principal Component Analysis (PCA)
===
class: smaller 

* Goal: PCA seeks to simplify a multi-dimensional (e.g. one with 3+ predictors (features)) dataset
* Uses 
 * Feature (predictor) extraction 
 * Can help reveal clusters 
 * May validate clustering results 
 
* Example: We have a dataset with 20 dimensions and it may be interesting to plot the data in two dimensions

What are Principal Components?
===
* Each "principal component" (PC) is an axis that captures the most variance
 * Variance is a measure of the spread of data points; standard deviation is the square root of variance 
 * Each PC is a combination of the original variables scaled by a coefficient
 * Every PC explains some variance
 * Each additional PC explains less variance than the previous one 

PCA Example 
===
class: smaller-75

* Using scale=TRUE is advisable
* `prcomp` first transforms the data by centering and scaling 
 * Centering is done by subtracting the column means
 * Scaling is done by dividing the (centered) columns of $x$ by their standard deviations
 
```{r}
iris_data <- iris[, 1:4]

pcaResult <- prcomp(iris_data, scale=TRUE)
summary(pcaResult)
```

Recovering the Original Data
===
class: smaller-50

```{r}
pcaResult$rotation
```

```{r}
# Original 
iris_data[1,]

# Recovered
tmp <- t(t(pcaResult$x %*% t(pcaResult$rotation)) * pcaResult$scale + pcaResult$center)
tmp[1,]
``` 

PCA Example Plots
===
class: smaller-75

```{r, fig.align="center", fig.height=4, fig.width=4, dpi=150, out.height="400px"}
# First 2 principal components (PC)
plot(pcaResult$x, pch=21, bg=c("red","green3","blue")[unclass(iris$Species)]) 
```
***
```{r, fig.align="center", fig.height=4, fig.width=4, dpi=150, out.height="400px"}
# PC variances
plot(pcaResult, type="line", cex.lab=1.5, cex.main=1.5, main="")
abline(h=1, lty=3, col="red") 
```

Nature of PCs Using biplot
===
class: smaller-50

* Close together points are similar 
* Close together vectors are similar

```{r, echo=FALSE}
par(mai=c(0.8,0.5,0.5,0))
```
```{r, fig.align="center", fig.height=4, fig.width=4, dpi=150, out.height="500px"}

biplot(pcaResult, scale=0, cex=.7)
``` 

Correlations between Vectors
===
class: smaller 

```{r}
cor(iris[,"Petal.Length"], iris[,"Petal.Width"])
cor(iris[,"Petal.Length"], iris[,"Sepal.Width"])
```

How Many Principal Components Should be Kept? 
===
* Kaiser criterion
 * Retain only principal components that with a variance greater than 1
 * Simple, but less advisable
* Scree Test
 * Find the place where the smooth decrease in the variances levels off
 * Multiple users may interpret the data different, unless trained the same

Getting Help
===
* Cross-Validated Stats Exchange
 * Part of Stack Overflow
 * http://stats.stackexchange.com/
* Biostars
 * https://www.biostars.org

===
